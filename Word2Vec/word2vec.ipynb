{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "**This is an Educational Notebook that teaches Word2Vec internal with documenation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Text Preprocessing\n",
    "\n",
    "**A simple corpus is tokenized into induvidual words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "    In machine learning, word embedding are a type of word representation.\n",
    "    that allows words to be represented as a vectors in a continous vector space.\n",
    "    Word embeddings are low dimensional floating point vectors.\n",
    "    The position of a word in the embedding space is learned from text and is based on the words that surround the word when it is used.\n",
    "    Word embeddings are calculated using an algorithm like Word2Vec.\n",
    "    Once calculated, word embeddings can be used as inputs to machine learning models.\n",
    "    Word embeddings can also be used to calculate things like word similarity.\n",
    "\"\"\".lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary from the corpus\n",
    "vocab = list(set(corpus))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a skip-gram training data: (center_word, context_word) pairs\n",
    "window_size = 2\n",
    "data = []\n",
    "\n",
    "for i, word in enumerate(corpus):\n",
    "    for j in range(max(i - window_size, 0), min(i + window_size + 1, len(corpus))):\n",
    "        if j != i:\n",
    "            data.append((word, corpus[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for one-hot encoding\n",
    "def one_hot(idx, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[idx] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Model Definition\n",
    "\n",
    "**Initializing embedding matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "W1 = np.random.randn(vocab_size, embedding_dim) # input embedding\n",
    "W2 = np.random.randn(embedding_dim, vocab_size) # output embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Activation & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sigmoid activation for binary classification*\n",
    ">$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Softmax activation for multiclass context-prediction\n",
    ">$$\n",
    "\\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$\n",
    "\n",
    "so both of these *Activation functions are used directly*\n",
    "\n",
    "Negative sampling loss\n",
    ">$$\n",
    "L = - \\log \\sigma(u_o^T v_c) - \\sum_{k \\in K} \\log \\sigma(-u_k^T v_c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# Negative sampling loss function\n",
    "def get_negative_samples(\n",
    "    pos_idx,\n",
    "    num_neg  \n",
    "):\n",
    "    \n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < num_neg:\n",
    "        neg = random.randint(0, vocab_size - 1)\n",
    "        if neg != pos_idx:\n",
    "            neg_samples.append(neg)\n",
    "            \n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Training Loop\n",
    "\n",
    "**Objective: Maximize the probability of context words given a center word**\n",
    "\n",
    "*The loss is used is categorical cross-entropy:*\n",
    "\n",
    "$$\n",
    "L = -\\sum_{k=1}^K \\log P(w_k | w_{center})\n",
    "$$\n",
    "\n",
    "where $K$ is the context window size and $w_k$ is the $kth$ context word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dl_dscore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# step 3: backward pass (gradients)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Gradient of loss w.r.t. score\u001b[39;00m\n\u001b[32m     23\u001b[39m dl_score = y_pred.copy()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mdl_dscore\u001b[49m[context_idx] -= \u001b[32m1\u001b[39m \u001b[38;5;66;03m# Subtract 1 from the true context index\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Gradient w.r.t W2 and W1\u001b[39;00m\n\u001b[32m     27\u001b[39m dW2 = np.outer(v_t, dl_dscore) \u001b[38;5;66;03m# (embedding_dim, vocab_size)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dl_dscore' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for target, context in data:\n",
    "        target_idx = word2idx[target]\n",
    "        context_idx = word2idx[context]\n",
    "        \n",
    "        # Step 1: Forward pass:\n",
    "        v_t = W1[target_idx]  # Target word vector (embedding_dim, 1)\n",
    "        score = np.dot(W2.T, v_t) # Raw score for all words (vocab_size, 1)\n",
    "        y_pred = softmax(score) # probability distribution over vocabulary (vocab_size, 1)\n",
    "        \n",
    "        # Step 2: compute loss\n",
    "        loss = -np.log(y_pred[context_idx])\n",
    "        total_loss += loss\n",
    "        \n",
    "        # step 3: backward pass (gradients)\n",
    "        # Gradient of loss w.r.t. score\n",
    "        dl_score = y_pred.copy()\n",
    "        dl_dscore[context_idx] -= 1 # Subtract 1 from the true context index\n",
    "        \n",
    "        # Gradient w.r.t W2 and W1\n",
    "        dW2 = np.outer(v_t, dl_dscore) # (embedding_dim, vocab_size)\n",
    "        dW1 = np.dot(W2, dl_dscore) # (embedding_dim,)\n",
    "        \n",
    "        # Step 4: update weights\n",
    "        W1[target_idx] -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Evaluation and Similarity\n",
    "\n",
    "**Functions to retrieve embeddings and compute cosine similarity between them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    return W1[word2idx[word]]\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def most_similar(word, top_n = 3):\n",
    "    vec = get_embedding(word)\n",
    "    sims = {}\n",
    "    for w in vocab:\n",
    "        if w != word:\n",
    "            sims[w] = cosine_similarity(vec, get_embedding(w))\n",
    "    \n",
    "    return sorted(sims.items(), key = lambda x: x[1], reverse = True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
