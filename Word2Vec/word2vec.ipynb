{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "**This is an Educational Notebook that teaches Word2Vec internal with documenation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Text Preprocessing\n",
    "\n",
    "**A simple corpus is tokenized into induvidual words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "    In machine learning, word embedding are a type of word representation.\n",
    "    that allows words to be represented as a vectors in a continous vector space.\n",
    "    Word embeddings are low dimensional floating point vectors.\n",
    "    The position of a word in the embedding space is learned from text and is based on the words that surround the word when it is used.\n",
    "    Word embeddings are calculated using an algorithm like Word2Vec.\n",
    "    Once calculated, word embeddings can be used as inputs to machine learning models.\n",
    "    Word embeddings can also be used to calculate things like word similarity.\n",
    "\"\"\".lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary from the corpus\n",
    "vocab = list(set(corpus))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a skip-gram training data: (center_word, context_word) pairs\n",
    "window_size = 2\n",
    "data = []\n",
    "\n",
    "for i, word in enumerate(corpus):\n",
    "    for j in range(max(i - window_size, 0), min(i + window_size + 1, len(corpus))):\n",
    "        if j != i:\n",
    "            data.append((word, corpus[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for one-hot encoding\n",
    "def one_hot(idx, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[idx] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Model Definition\n",
    "\n",
    "**Initializing embedding matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "W1 = np.random.randn(vocab_size, embedding_dim) # input embedding\n",
    "W2 = np.random.randn(embedding_dim, vocab_size) # output embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Activation & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sigmoid activation for binary classification*\n",
    ">$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Softmax activation for multiclass context-prediction\n",
    ">$$\n",
    "\\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$\n",
    "\n",
    "so both of these *Activation functions are used directly*\n",
    "\n",
    "Negative sampling loss\n",
    ">$$\n",
    "L = - \\log \\sigma(u_o^T v_c) - \\sum_{k \\in K} \\log \\sigma(-u_k^T v_c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# Negative sampling loss function\n",
    "def get_negative_samples(\n",
    "    pos_idx,\n",
    "    num_neg  \n",
    "):\n",
    "    \n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < num_neg:\n",
    "        neg = random.randint(0, vocab_size - 1)\n",
    "        if neg != pos_idx:\n",
    "            neg_samples.append(neg)\n",
    "            \n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Training Loop\n",
    "\n",
    "**Objective: Maximize the probability of context words given a center word**\n",
    "\n",
    "*The loss is used is categorical cross-entropy:*\n",
    "\n",
    "$$\n",
    "L = -\\sum_{k=1}^K \\log P(w_k | w_{center})\n",
    "$$\n",
    "\n",
    "where $K$ is the context window size and $w_k$ is the $kth$ context word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for target, context in data:\n",
    "        target_idx = word2idx[target]\n",
    "        context_idx = word2idx[context]\n",
    "        \n",
    "        # Step 1: Forward pass:\n",
    "        v_t = W1[target_idx]  # Target word vector (embedding_dim, 1)\n",
    "        score = np.dot(W2.T, v_t) # Raw score for all words (vocab_size, 1)\n",
    "        y_pred = softmax(score)\n",
    "        \n",
    "        # Step 2: compute loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
